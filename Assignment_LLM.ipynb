{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0079de7-f91c-4936-bc68-9dd8eb841929",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9f169-ab0b-40b7-9155-79d2f8184066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7367c-1aa1-4912-81e0-f4a8e4ae0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (helps with displaying dataframes containing long strings)\n",
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d84e0-4b17-4d50-acc6-b02f488bf3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key\n",
    "openai.api_key = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b0a6c2-7244-4ce8-b9e6-c53e6ec82009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic clinical notes (1% sample of ~158K)\n",
    "# See https://huggingface.co/datasets/starmpcc/Asclepius-Synthetic-Clinical-Notes for details\n",
    "ds = load_dataset(\"starmpcc/Asclepius-Synthetic-Clinical-Notes\", split=\"train[:1%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fa2eef-f1e0-4fc0-8287-1995485d0396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to Question Answering\n",
    "ds_qa = ds.filter(lambda ex: ex[\"task\"] == \"Question Answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b231646d-5165-4502-80b0-52c796275350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set maximum number of examples to run\n",
    "N_EXAMPLES = ds_qa.num_rows\n",
    "print(N_EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8488926-8d0a-42b7-a093-6dc96508115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through examples and record ChatGPT answers\n",
    "new_answers_1 = []\n",
    "for i in range(0, N_EXAMPLES):\n",
    "    # Get notes and question\n",
    "    temp_notes = ds_qa[\"note\"][i]\n",
    "    temp_question = ds_qa[\"question\"][i]\n",
    "\n",
    "    # Structure prompt\n",
    "    temp_prompt = f\"\"\"\n",
    "        Answer the following question given the context below:\n",
    "        {temp_question}\n",
    "        \n",
    "        \"{temp_notes}\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Run through OpenAI\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": temp_prompt},\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Append to list\n",
    "    new_answers_1.append(response.choices[0].message[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7961a63c-acc1-4b17-88c2-f4d945093e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe and save temporary copy\n",
    "df_1 = pd.DataFrame({\n",
    "    \"note\": ds_qa[\"note\"][0:N_EXAMPLES],\n",
    "    \"question\": ds_qa[\"question\"][0:N_EXAMPLES],\n",
    "    \"answer\": ds_qa[\"answer\"][0:N_EXAMPLES],\n",
    "    \"new_answer\": new_answers_1\n",
    "})\n",
    "df_1.to_csv(\"temp_llm_answers_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73566b5-e193-437b-833b-fa195a20f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simple tokenizer (to use with calculating F1 score)\n",
    "def simple_tokenize(string):\n",
    "    # Remove non-alphanumeric\n",
    "    string = re.sub(\"[^0-9a-zA-Z]+\", \" \", string)\n",
    "    string = re.sub(r\"\\s+\", \" \", string)\n",
    "    \n",
    "    # Lowercase and split to list\n",
    "    return string.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eadb02-bc23-4b3d-8bf4-2d9f6abdbaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to calculate F1 score for two strings\n",
    "def calc_f1_score(str1, str2):\n",
    "    # Tokenize the strings\n",
    "    tokens1 = simple_tokenize(str1)\n",
    "    tokens2 = simple_tokenize(str2)\n",
    "    \n",
    "    # Create token counters\n",
    "    counter1 = Counter(tokens1)\n",
    "    counter2 = Counter(tokens2)\n",
    "    \n",
    "    # Calculate the number of common tokens\n",
    "    common_tokens = sum((counter1 & counter2).values())\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = common_tokens / len(tokens2) if tokens2 else 0\n",
    "    recall = common_tokens / len(tokens1) if tokens1 else 0\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    if precision + recall == 0:\n",
    "        f1_score = 0\n",
    "    else:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828cddcd-c9d6-4395-91cc-1fa7dfeb6779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 score for original vs new answers\n",
    "f1_scores = []\n",
    "for i in range(0, df_1.shape[0]):\n",
    "    f1_scores.append(calc_f1_score(df_1[\"answer\"][i], df_1[\"new_answer\"][i]))\n",
    "\n",
    "df_1[\"f1_score\"] = f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b248c4-67fa-4968-8656-c525117e5e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.sort_values(\"f1_score\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3b2079-e36e-491e-971a-7d6ea8ea7e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.sort_values(\"f1_score\").tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26948b67-e808-4d02-896c-7c3b51e703b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_1[\"f1_score\"])\n",
    "plt.xlabel(\"F1 Score\")\n",
    "plt.ylabel(\"Number of Examples\")\n",
    "print(f\"Mean F1 score: {round(df_1['f1_score'].mean(), 2)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1af7a4-68b9-4c8f-9986-17d55a9646b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare average string length\n",
    "print(f\"Mean # of characters for original answers: {np.mean([len(x) for x in df_1['answer']])}\")\n",
    "print(f\"Mean # of characters for new answers: {np.mean([len(x) for x in df_1['new_answer']])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b885f967-1c6e-4ae0-adfa-9db0a046f5e6",
   "metadata": {},
   "source": [
    "# Part 2: Re-run with more instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cece96-f953-45f6-ac0a-526167d43329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through examples and record ChatGPT answers\n",
    "new_answers_2 = []\n",
    "for i in range(1, N_EXAMPLES):\n",
    "    # Get notes and question\n",
    "    temp_notes = ds_qa[\"note\"][i]\n",
    "    temp_question = ds_qa[\"question\"][i]\n",
    "\n",
    "    # Structure prompt\n",
    "    temp_prompt = f\"\"\"\n",
    "        Here is an example of a question, context, and correct answer:\n",
    "        Question: \"{ds_qa[\"note\"][0]}\"\n",
    "        Context: {ds_qa[\"note\"][0]}\n",
    "        Answer: {ds_qa[\"answer\"][0]}\n",
    "\n",
    "        \n",
    "        Now answer the following question given the context below; keep the answer concise:\n",
    "        {temp_question}\n",
    "        \n",
    "        \"{temp_notes}\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Run through OpenAI\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": temp_prompt},\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Append to list\n",
    "    new_answers_2.append(response.choices[0].message[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6550559f-c7d2-4774-b9b9-c28c1afffe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe and save temporary copy\n",
    "df_2 = pd.DataFrame({\n",
    "    \"note\": ds_qa[\"note\"][1:N_EXAMPLES],\n",
    "    \"question\": ds_qa[\"question\"][1:N_EXAMPLES],\n",
    "    \"answer\": ds_qa[\"answer\"][1:N_EXAMPLES],\n",
    "    \"new_answer\": new_answers_2\n",
    "})\n",
    "df_2.to_csv(\"temp_llm_answers_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b9764-ec90-4ac5-955e-c3c924477785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 score for original vs new answers\n",
    "f1_scores = []\n",
    "for i in range(0, df_2.shape[0]):\n",
    "    f1_scores.append(calc_f1_score(df_2[\"answer\"][i], df_2[\"new_answer\"][i]))\n",
    "\n",
    "df_2[\"f1_score\"] = f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb5ed69-867a-4864-927a-2fbd498a4f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_2[\"f1_score\"])\n",
    "plt.xlabel(\"F1 Score\")\n",
    "plt.ylabel(\"Number of Examples\")\n",
    "print(f\"Mean F1 score: {round(df_2['f1_score'].mean(), 2)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b581295f-bff2-4bd7-abf8-53166a475594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare average string length\n",
    "print(f\"Mean # of characters for original answers: {np.mean([len(x) for x in df_2['answer']])}\")\n",
    "print(f\"Mean # of characters for new answers: {np.mean([len(x) for x in df_2['new_answer']])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ff7f1-2588-4f0e-aa57-cef9f88d5374",
   "metadata": {},
   "source": [
    "# Part 3: Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89297dc5-6e93-4a79-8d59-c328e49c6251",
   "metadata": {},
   "source": [
    "Use ChatGPT to assign a severity score 1-10 to each example, where 10 is most severe. Ask the model to explain its reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64572ec-2101-43ad-b429-8788e7520a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through examples and record ChatGPT answers\n",
    "new_answers_3 = []\n",
    "for i in range(0, N_EXAMPLES):\n",
    "    # Get notes and question\n",
    "    temp_notes = ds_qa[\"note\"][i]\n",
    "\n",
    "    # Structure prompt\n",
    "    temp_prompt = f\"\"\"\n",
    "        Based on the notes below, rate the patient's level of illness on a score from 1 to 10, where 10 is the most severe.\n",
    "        Please explain your reasoning in one sentence.\n",
    "        Answers should be structured as \"Score: __; Reasoning: __\"\n",
    "        \n",
    "        Notes: \"{temp_notes}\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Run through OpenAI\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a medical professional reviewing clinical notes.\"},\n",
    "            {\"role\": \"user\", \"content\": temp_prompt},\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Append to list\n",
    "    new_answers_3.append(response.choices[0].message[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1251b0ab-4a72-464f-957f-6f360446a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe and save temporary copy\n",
    "df_3 = pd.DataFrame({\n",
    "    \"note\": ds_qa[\"note\"][0:N_EXAMPLES],\n",
    "    \"score\": [int(re.search(r\"Score:\\s*(\\d+)\", x).group(1)) for x in new_answers_3],\n",
    "    \"reasoning\": [re.search(r\"Reasoning:\\s*(.*)\", x).group(1) for x in new_answers_3],\n",
    "})\n",
    "df_2.to_csv(\"temp_llm_answers_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b94630-2ae8-4d84-9415-a5ac112038c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.sort_values(\"score\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3bd426-f04d-4792-ab2b-115ca136e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_3[\"score\"])\n",
    "plt.xlabel(\"Illness Severity Score\")\n",
    "plt.ylabel(\"Number of Examples\")\n",
    "print(f\"Mean severity score: {round(df_3['score'].mean(), 2)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a443c7c-5c11-4549-a0bc-5fd2544ad3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
